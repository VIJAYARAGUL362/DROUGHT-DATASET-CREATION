# -*- coding: utf-8 -*-
"""Drought Dataset

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15b2aV_SPN0DYBB3VU08fUEr8jSKuZsAJ

# IMPORTING LIBRARIES
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import xarray as xr

"""# IMPORTING THE DATASET"""

training_dataframe = xr.open_dataset('spei03.nc')
training_dataframe.variables

"""# DATA EXPLORATION"""

training_dataframe['spei']

training_dataframe['spei'].shape

training_dataframe['spei'].isel(time=0, lat=100, lon=200).values

# CHECKING FOR CHENNAI ON MARCH 2023
val = training_dataframe['spei'].sel(time="2020-03", lat=13.0, lon=80.0, method="nearest").values
print(val)

"""# DATA SELECTION"""

# SELECTING ONLY THE TIME FRAME (2015-2023)
dataset = training_dataframe.sel(time=slice("2015-01-01", "2023-12-31"))

dataset.sizes

spie = dataset['spei']

# SHAPE
total_values = np.prod(spie.shape)

# COUNT OF NOT NULL VALUES
count_not_null = np.count_nonzero(~np.isnan(spie.values))

total_values,count_not_null

# CONVERTING THE NETCDF TO PANDAS DATAFRAME
not_null_dataset = dataset.to_dataframe().reset_index().dropna()

not_null_dataset.columns

not_null_dataset

"""# DATA EXPLORATION"""

not_null_dataset.info()

not_null_dataset.describe()

"""## BINNING FOR CLASSIFICATION (OCCURED)"""

not_null_dataset['label'] = (not_null_dataset['spei']< -1.0).astype(int)

not_null_dataset['label'].value_counts()

"""## SPATIALTEMPORAL SAMPLING"""

# COPYING THE DATASET
sampling_dataset = not_null_dataset.copy()

sampling_dataset

"""## CREATING BINS"""

# MONTHS
sampling_dataset['month'] = sampling_dataset['time'].dt.month

# LAT AND LONG
sampling_dataset['lat_bin'] = pd.cut(sampling_dataset['lat'],bins=np.arange(-90,91,5))
sampling_dataset['lon_bin'] = pd.cut(sampling_dataset['lon'],bins=np.arange(-90,91,5))

sampling_dataset

# GROUPING BY SPATIOTEMPORAL STRATA
groups = sampling_dataset.groupby(['month','lat_bin','lon_bin'],observed=True)

balanced_samples = []
ratio = 1

# CHECKING EVERY DROUGHT AND NO DROUGHT IS BALANCED FOR THE MONTH AND THE LOCATION
for i,g in groups:
    events = g[g['label']==1]
    non_events = g[g['label']==0]
    if len(events) > 0 and len(non_events) > 0:
        n = min(len(events),len(non_events))*ratio
        balanced = pd.concat([
            events.sample(n=min(len(events),n),random_state=42),
            non_events.sample(n=min(len(non_events),n),random_state=42)
            ])
    balanced_samples.append(balanced)

df_balanced = pd.concat(balanced_samples).sample(frac=1,random_state=42)

df_balanced['label'].value_counts()

df_balanced[['month','label']].value_counts()

"""## VISUALIZING THE DISTRUBUTION OF THE GROUPING BASED ON LACATION"""

! pip install cartopy

import matplotlib.pyplot as plt

region_counts = df_balanced.groupby(['lat_bin', 'lon_bin']).size().reset_index(name="count")

plt.scatter(region_counts['lon_bin'].astype(str),
            region_counts['lat_bin'].astype(str),
            s=region_counts['count']/100, alpha=0.5)
plt.xticks(rotation=90)
plt.title("Geographic coverage after balancing")
plt.show()

import matplotlib.pyplot as plt
import cartopy.crs as ccrs
import cartopy.feature as cfeature

# Convert bins to midpoints for plotting
def midpoint(interval):
    return (interval.left + interval.right) / 2

region_counts = df_balanced.groupby(['lat_bin', 'lon_bin']).size().reset_index(name="count")
region_counts["lat"] = region_counts["lat_bin"].apply(midpoint)
region_counts["lon"] = region_counts["lon_bin"].apply(midpoint)

# Plot world map
fig = plt.figure(figsize=(12,6))
ax = plt.axes(projection=ccrs.PlateCarree())
ax.set_global()
ax.coastlines()
ax.add_feature(cfeature.BORDERS, linewidth=0.5)
ax.add_feature(cfeature.LAND, facecolor="lightgray", alpha=0.5)

# Scatter plot (size ~ sample count in each bin)
sc = ax.scatter(
    region_counts["lon"], region_counts["lat"],
    s=region_counts["count"]*0.01,  # adjust scaling factor for visibility
    color="steelblue", alpha=0.6, transform=ccrs.PlateCarree()
)

plt.title("Geographic coverage after balancing", fontsize=14)
plt.show()

# DROPING CRS
df_balanced.drop('crs',inplace=True,axis=1)

df_balanced

df_balanced.to_csv('stage_1_drought_dataset.csv',index=False)

"""# STAGE 1 DATASET

## IMPORTING THE DATASET
"""

training_dataframe_1 = pd.DataFrame(pd.read_csv('stage_1_drought_dataset.csv'))

training_dataframe_1.info()

training_dataframe_1.head()

training_dataframe_1['label'].value_counts()

"""## SAMPLING THE DATASET"""

import datetime as dt

# TIME
training_dataframe_1['date'] = pd.to_datetime(training_dataframe_1['time'])

training_dataframe_1['year'] = training_dataframe_1['date'].dt.year

training_dataframe_1.info()

"""## REDUCING COUNT OF EACH TRAINING SAMPLE IN EACH GROUP OF LAT,LON AND MONTH"""

import pandas as pd

# target size per (month, label)
target_per_group = 5000

df_reduced = (
    training_dataframe_1.groupby(['month', 'label'], group_keys=False)
      .apply(lambda g: g.sample(n=min(len(g), target_per_group), random_state=42))
      .reset_index(drop=True)
)

print("Reduced size:", len(df_reduced))
print(df_reduced['label'].value_counts())

df_reduced

df_reduced[['month','label']].value_counts()

df_reduced.to_csv('stage_2_drought_dataset.csv',index=False)

"""# STAGE 2 DATASET

## IMPORT LIBRARIES
"""

import pandas as pd
import numpy as np

"""## IMPORTING THE DATASET"""

dataset_2 = pd.read_parquet('merged_file.parquet')

stage_2 = pd.DataFrame(pd.read_csv('/content/stage_2_drought_dataset.csv'))

training_dataframe_2 = pd.DataFrame(dataset_2)

training_dataframe_2

"""## MERGING THE DATASET THE STAGE 1"""

stage_2

stage_2.iloc(axis=0)[113999]

training_dataframe_2.loc[training_dataframe_2['row_id'] == 113999]

stage_2.reset_index(inplace=True,names=['row_id'])

stage_2

merged_dataset = pd.merge(left=training_dataframe_2,right=stage_2,on='row_id',how='inner')

merged_dataset

merged_dataset.columns

merged_dataset.drop(columns=[ 'time_y', 'lat_y','lon_y'],inplace=True)

merged_dataset.columns

merged_dataset

merged_dataset.to_csv('stage_3_drought_dataset.csv',index=False)

"""# STAGE 3 DATASET"""

dataset_3 = pd.read_csv('stage_3_drought_dataset.csv')
training_dataframe_3 = pd.DataFrame(dataset_3)

training_dataframe_3

"""### FEATURE ENGINEERING"""

import numpy as np

# Latitude & Longitude sinusoidal encoding
training_dataframe_3["lat_sin"] = np.sin(np.pi * training_dataframe_3["lat_x"] / 180)
training_dataframe_3["lat_cos"] = np.cos(np.pi * training_dataframe_3["lat_x"] / 180)
training_dataframe_3["lon_sin"] = np.sin(np.pi * training_dataframe_3["lon_x"] / 180)
training_dataframe_3["lon_cos"] = np.cos(np.pi * training_dataframe_3["lon_x"] / 180)

# Month cyclic encoding
training_dataframe_3["month_sin"] = np.sin(2 * np.pi * training_dataframe_3["month"] / 12)
training_dataframe_3["month_cos"] = np.cos(2 * np.pi * training_dataframe_3["month"] / 12)

training_dataframe_3

training_dataframe_3.columns

"""## DATA SELECTION"""

# DROPPING THE UNWANTED COLUMNS
training_dataframe_3.drop(columns=['lat_x', 'lon_x', 'time_x','date', 'year','month'],inplace=True)

training_dataframe_3

training_dataframe_3.columns

training_dataframe_3.drop(columns=['lat_bin','lon_bin'],inplace=True)

training_dataframe_3

training_dataframe_3.to_csv('stage_4_drought_dataset.csv',index=False)

import pandas as pd

datset = pd.read_csv('/content/stage_4_drought_dataset.csv')

df = pd.DataFrame(datset)

df.to_excel("drought_dataset.xlsx",index=False)

